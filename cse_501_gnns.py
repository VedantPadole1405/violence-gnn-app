# -*- coding: utf-8 -*-
"""CSE 501 - GNNS.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dxOlMEB9TpohTHi5WjvfVoaKub6kbT9k
"""

import pandas as pd
import numpy as np

file_path = "/content/Violence-Project-Mass-Shooter-Database-Version-5-May-2022 (2).xlsx"

df = pd.read_excel(file_path, sheet_name="Full Database")

print(df.shape)
df.head()
print(df.columns.tolist())

df

import pandas as pd
import numpy as np

file_path = "/content/Violence-Project-Mass-Shooter-Database-Version-5-May-2022 (2).xlsx"

# Use two header rows: row 0 (section names), row 1 (variable names)
df_raw = pd.read_excel(file_path, sheet_name="Full Database", header=[0, 1])

print("Raw shape:", df_raw.shape)
print("Raw multiindex columns example:", df_raw.columns[:10])

import pandas as pd
import numpy as np

file_path = "/content/Violence-Project-Mass-Shooter-Database-Version-5-May-2022 (2).xlsx"

df_raw = pd.read_excel(file_path, sheet_name="Full Database", header=[0,1])

# Flatten
flat_cols = []
for top, sub in df_raw.columns:
    top = str(top).strip()
    sub = str(sub).strip()
    if top and sub:
        flat_cols.append(f"{top}__{sub}")
    elif sub:
        flat_cols.append(sub)
    else:
        flat_cols.append(top)

df_raw.columns = flat_cols

print("Shape:", df_raw.shape)
print("Example columns:", df_raw.columns[:15].tolist())

killed_candidates = [c for c in df_raw.columns if "kill" in c.lower()]
print("Killed variable candidates:", killed_candidates)

# Should be: 'Victims__Number Killed'
killed_col = killed_candidates[0]
print("Using:", killed_col)

df = df_raw.copy()

# Remove rows where number killed is missing
df = df.dropna(subset=[killed_col])

# Label = high lethality (above median)
median_k = df[killed_col].median()
df["high_lethality"] = (df[killed_col] > median_k).astype(int)

print("Median killed:", median_k)
df[[killed_col, "high_lethality"]].head()

from sklearn.preprocessing import StandardScaler

# Select numeric columns only
numeric_df = df.select_dtypes(include=[np.number]).copy()

# Drop index-like identifiers that shouldn’t be used as features
drop_cols = [c for c in numeric_df.columns if "case" in c.lower()]
if len(drop_cols):
    numeric_df = numeric_df.drop(columns=drop_cols)
    print("Dropped:", drop_cols)

print("Numeric feature count:", numeric_df.shape[1])

X_raw = numeric_df.fillna(0).values

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_raw)

import torch
x = torch.tensor(X_scaled, dtype=torch.float)
y = torch.tensor(df["high_lethality"].values, dtype=torch.long)

num_nodes = x.shape[0]
num_nodes

# Detect all possible grouping columns
city_cols = [c for c in df.columns if "location__city" in c.lower()]
state_cols = [c for c in df.columns if "location__state" in c.lower()]
loc_type_cols = [c for c in df.columns if "location__" in c.lower() and "type" in c.lower()]
year_cols = [c for c in df.columns if "date__year" in c.lower()]

print("City cols:", city_cols)
print("State cols:", state_cols)
print("LocationType cols:", loc_type_cols)
print("Year cols:", year_cols)

!pip install torch_scatter torch_sparse torch_cluster torch_spline_conv torch_geometric -f https://data.pyg.org/whl/torch-2.9.0+cu124.html

from torch_geometric.data import Data

edge_index_list = []

def fully_connect(indices):
    ids = list(indices)
    for i in range(len(ids)):
        for j in range(i+1, len(ids)):
            edge_index_list.append([ids[i], ids[j]])
            edge_index_list.append([ids[j], ids[i]])

# 1. CITY EDGES
city_col = 'Location__City'

for city, idxs in df.groupby(city_col).groups.items():
    if len(idxs) > 1:
        fully_connect(idxs)

print("Added city edges")

# 2. STATE EDGES
state_col = 'Location__State'

for st, idxs in df.groupby(state_col).groups.items():
    if len(idxs) > 1:
        fully_connect(idxs)

print("Added state edges")

# 3. STATE CODE EDGES (OPTIONAL)
state_code_col = 'Location__State Code'

for sc, idxs in df.groupby(state_code_col).groups.items():
    if len(idxs) > 1:
        fully_connect(idxs)

print("Added state code edges")

# 4. DECADE EDGES
year_col = 'Date__Year'
df["Decade"] = (df[year_col] // 10) * 10

for dec, idxs in df.groupby("Decade").groups.items():
    if len(idxs) > 1:
        fully_connect(idxs)

print("Added decade edges")

import torch

edge_index = torch.tensor(edge_index_list, dtype=torch.long).t().contiguous()

print("Nodes:", num_nodes)
print("Edges:", edge_index.shape[1])

from torch_geometric.data import Data

data = Data(x=x, edge_index=edge_index, y=y)

data

# edge_index is [2, num_edges]
edge_list = list(zip(edge_index[0].tolist(), edge_index[1].tolist()))

# Print first 20 edges
edge_list[:20]

city_edges = []
state_edges = []
state_code_edges = []
decade_edges = []

# Build tracking versions of fully_connect
def fully_connect_track(indices, store_list):
    ids = list(indices)
    for i in range(len(ids)):
        for j in range(i+1, len(ids)):
            store_list.append((ids[i], ids[j]))
            store_list.append((ids[j], ids[i]))

city = "Los Angeles"

idxs = df.index[df["Location__City"] == city].tolist()
print("Nodes in this city:", idxs)

# Extract induced subgraph edges
sub_edges = []
for (u, v) in edge_list:
    if u in idxs and v in idxs:
        sub_edges.append((u, v))

print("Edges in this city subgraph:", sub_edges[:10])

from sklearn.manifold import TSNE
import matplotlib.pyplot as plt

emb = TSNE(n_components=2).fit_transform(x.cpu().numpy())

plt.figure(figsize=(8,8))
plt.scatter(emb[:,0], emb[:,1], s=20)
plt.title("TSNE Visualization of Shooter Node Features")
plt.show()

edge_index[ : , :20 ]

from sklearn.model_selection import train_test_split
import numpy as np
import torch

num_nodes = x.shape[0]

indices = np.arange(num_nodes)

train_idx, test_idx = train_test_split(
    indices, test_size=0.2, random_state=42, stratify=y.numpy()
)
train_idx, val_idx = train_test_split(
    train_idx, test_size=0.2, random_state=42, stratify=y[train_idx].numpy()
)

train_mask = torch.zeros(num_nodes, dtype=torch.bool)
val_mask   = torch.zeros(num_nodes, dtype=torch.bool)
test_mask  = torch.zeros(num_nodes, dtype=torch.bool)

train_mask[train_idx] = True
val_mask[val_idx]     = True
test_mask[test_idx]   = True

data

num_nodes = data.num_nodes  # should be 181

train_mask = torch.zeros(num_nodes, dtype=torch.bool)
val_mask   = torch.zeros(num_nodes, dtype=torch.bool)
test_mask  = torch.zeros(num_nodes, dtype=torch.bool)

train_mask[train_idx] = True
val_mask[val_idx]     = True
test_mask[test_idx]   = True

data.train_mask = train_mask
data.val_mask   = val_mask
data.test_mask  = test_mask

data

import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import GCNConv

class GCNClassifier(nn.Module):
    def __init__(self, in_channels, hidden_channels, num_classes, dropout=0.5):
        super(GCNClassifier, self).__init__()
        self.conv1 = GCNConv(in_channels, hidden_channels)
        self.conv2 = GCNConv(hidden_channels, hidden_channels)
        self.lin   = nn.Linear(hidden_channels, num_classes)
        self.dropout = dropout

    def forward(self, data):
        x = data.x
        edge_index = data.edge_index

        x = self.conv1(x, edge_index)
        x = F.relu(x)
        x = F.dropout(x, p=self.dropout, training=self.training)

        x = self.conv2(x, edge_index)
        x = F.relu(x)
        x = F.dropout(x, p=self.dropout, training=self.training)

        logits = self.lin(x)
        return logits

in_channels  = data.num_node_features   # 109
hidden_size  = 64
num_classes  = 2                        # high_lethality: 0/1

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

model = GCNClassifier(in_channels, hidden_size, num_classes, dropout=0.5).to(device)
data  = data.to(device)

criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)

def train_epoch():
    model.train()
    optimizer.zero_grad()
    out  = model(data)
    loss = criterion(out[data.train_mask], data.y[data.train_mask])
    loss.backward()
    optimizer.step()
    return loss.item()

@torch.no_grad()
def accuracy(mask):
    model.eval()
    out   = model(data)
    preds = out[mask].argmax(dim=1)
    labels = data.y[mask]
    correct = (preds == labels).sum().item()
    total   = int(mask.sum().item())
    if total == 0:
        return 0.0
    return correct / total

num_epochs = 200

for epoch in range(1, num_epochs + 1):
    loss = train_epoch()
    if epoch == 1 or epoch % 20 == 0:
        train_acc = accuracy(data.train_mask)
        val_acc   = accuracy(data.val_mask)
        print(f"Epoch {epoch:03d} | Loss: {loss:.4f} | "
              f"Train Acc: {train_acc:.3f} | Val Acc: {val_acc:.3f}")

test_acc = accuracy(data.test_mask)
print("Final Test Accuracy:", round(test_acc, 3))

"""PCA + GraphSage

"""

from sklearn.decomposition import PCA
import torch
import numpy as np

# X is your (181 × 109) feature matrix
X = data.x.cpu().numpy()

pca = PCA(n_components=32)
X_pca = pca.fit_transform(X)

data.x = torch.tensor(X_pca, dtype=torch.float)
data.x.shape

from sklearn.neighbors import kneighbors_graph
import torch

# Build knn adjacency (sparse matrix)
A = kneighbors_graph(X_pca, n_neighbors=8, mode='connectivity', include_self=False)
A = A.toarray()

edge_index = torch.tensor(np.vstack(np.nonzero(A)), dtype=torch.long)
data.edge_index = edge_index

data.edge_index.shape

torch.manual_seed(42)
num_nodes = data.num_nodes

idx = torch.randperm(num_nodes)
train_size = int(0.7 * num_nodes)
val_size = int(0.15 * num_nodes)

data.train_mask = torch.zeros(num_nodes, dtype=torch.bool)
data.val_mask = torch.zeros(num_nodes, dtype=torch.bool)
data.test_mask = torch.zeros(num_nodes, dtype=torch.bool)

data.train_mask[idx[:train_size]] = True
data.val_mask[idx[train_size:train_size+val_size]] = True
data.test_mask[idx[train_size+val_size:]] = True

import torch.nn as nn
from torch_geometric.nn import SAGEConv
import torch.nn.functional as F

class GraphSAGE(torch.nn.Module):
    def __init__(self, in_channels, hidden_channels, out_channels):
        super().__init__()
        self.conv1 = SAGEConv(in_channels, hidden_channels)
        self.conv2 = SAGEConv(hidden_channels, out_channels)

    def forward(self, x, edge_index):
        x = F.relu(self.conv1(x, edge_index))
        x = F.dropout(x, p=0.3, training=self.training)
        x = self.conv2(x, edge_index)
        return x

model = GraphSAGE(in_channels=32, hidden_channels=64, out_channels=4)
optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=1e-4)
criterion = nn.CrossEntropyLoss()

def accuracy(mask):
    pred = model(data.x, data.edge_index).argmax(dim=1)
    return (pred[mask] == data.y[mask]).float().mean().item()

def train_epoch():
    model.train()
    optimizer.zero_grad()
    out = model(data.x, data.edge_index)
    loss = criterion(out[data.train_mask], data.y[data.train_mask])
    loss.backward()
    optimizer.step()
    return loss.item()

for epoch in range(1, 201):
    loss = train_epoch()
    if epoch % 20 == 0 or epoch == 1:
        val_acc = accuracy(data.val_mask)
        print(f"Epoch {epoch:03d} | Loss: {loss:.4f} | Val Acc: {val_acc:.3f}")

test_acc = accuracy(data.test_mask)
print("Final Test Accuracy:", test_acc)

torch.save(model.state_dict(), "graphsage_model.pt")

X_pca = pca.fit_transform(X)

np.save("pca.npy", X_pca)

files.download("pca.npy")

df.to_csv("cleaned_features.csv", index=False)

A = kneighbors_graph(X_pca, n_neighbors=8, mode='connectivity').toarray()
edge_index = np.vstack(np.nonzero(A))  # shape = [2, num_edges]

np.save("knn_edges.npy", edge_index)

edge_index = np.load("knn_edges.npy")

